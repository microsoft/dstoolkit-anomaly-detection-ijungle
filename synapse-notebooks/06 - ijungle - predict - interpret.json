{
	"name": "06 - ijungle - predict - interpret",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "8d8beece-cf5b-4830-95bb-9732514b8bb5"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"from pyspark.ml.functions import vector_to_array\r\n",
					"import joblib\r\n",
					"import numpy as np"
				],
				"execution_count": 61
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"best_iforest_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/06-best_iforest'\r\n",
					"predict_prepped_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/07-Predict_Prepped_Data'\r\n",
					"results_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/08-Result_Data'\r\n",
					"features_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/02-Features'\r\n",
					"interpret_path = 'abfss://anomalydetectoroutput@eiadapstoreworking.dfs.core.windows.net/Results/iJungle_Output'\r\n",
					"id_feat = \"['issuer_id','issued_date']\"\r\n",
					"id_feat_types = \"['int', 'date']\"\r\n",
					"score_threshold = \"-0.8\""
				],
				"execution_count": 114
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Cast parameters\r\n",
					"id_feat = eval(id_feat)\r\n",
					"id_feat_types = eval(id_feat_types)\r\n",
					"score_threshold = float(score_threshold)"
				],
				"execution_count": 115
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_results = spark.read.parquet(results_path).where(F.col('score')<=score_threshold)\r\n",
					"print(\"Number of anomalies found: {:,}\".format(df_results.count()))"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_assembled = df_results.join(spark.read.parquet(predict_prepped_path), on=id_feat)\r\n",
					"print(\"Number assembled records: {:,}\".format(df_assembled.count()))"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"num_feats = len(df_assembled.head(1)[0]['scaled'])\r\n",
					"print(\"Number of features:\", num_feats)\r\n",
					"df_unassembled = df_assembled.withColumn('f', vector_to_array(\"scaled\")).select(id_feat + ['score'] + [F.col(\"f\")[i] for i in range(num_feats)])\r\n",
					"print(\"Number of records of overhead dataset: {:,}\".format(df_unassembled.count()))"
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"model_bytes = spark.read.parquet(best_iforest_path).head(1)[0]['model']\r\n",
					"with open('best_iforest.pkl', 'bw') as model_file:\r\n",
					"    model_file.write(model_bytes)\r\n",
					"clf = joblib.load('best_iforest.pkl')\r\n",
					"clf"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"feats = np.array([row['feat'] for row in spark.read.parquet(features_path).orderBy('id').collect()])\r\n",
					"feats.shape"
				],
				"execution_count": 63
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"feat3_lst = ['feat1', 'feat2', 'feat3']\r\n",
					"score3_lst = ['score1', 'score2', 'score3']\r\n",
					"feat_score_lst = list(np.array(list(zip(feat3_lst, score3_lst))).reshape(6))\r\n",
					"dcc_str = \", \".join([x[0]+\" \"+x[1] for x in zip(id_feat, id_feat_types)]) + \", score float, \"\r\n",
					"dcc_str += \", \".join([x[0]+\" \"+x[1] for x in zip(feat_score_lst, ['string', 'float', 'string', 'float', 'string', 'float'])])"
				],
				"execution_count": 110
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def ijungle_interpret(id_feat, clf, feats, feat3_lst, score3_lst):\r\n",
					"    def _fun(iterator):\r\n",
					"        import shap\r\n",
					"        explainer = shap.TreeExplainer(clf)\r\n",
					"        feat_score_lst = list(np.array(list(zip(feat3_lst, score3_lst))).reshape(6))\r\n",
					"        for pdf in iterator:\r\n",
					"            pdf.set_index(id_feat + ['score'], inplace=True)\r\n",
					"            shap_values = explainer.shap_values(pdf)\r\n",
					"            top_feats = feats[shap_values.argsort()[:,:3]]\r\n",
					"            top_scores = np.sort(shap_values)[:,:3]\r\n",
					"            pdf_out = pdf.reset_index()\r\n",
					"            pdf_out = pdf_out[id_feat + ['score']]\r\n",
					"            pdf_out[feat3_lst] = top_feats\r\n",
					"            pdf_out[score3_lst] = top_scores\r\n",
					"            pdf_out = pdf_out[id_feat + ['score'] + feat_score_lst]\r\n",
					"            yield(pdf_out)\r\n",
					"    return(_fun)"
				],
				"execution_count": 104
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df_result = df_unassembled.mapInPandas(ijungle_interpret(id_feat, clf, feats, feat3_lst, score3_lst),dcc_str)\r\n",
					"print(\"Number of anomalies with intepretation: {:,}\".format(df_result.count()))"
				],
				"execution_count": 113
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_results.write.mode('overwrite').parquet(interpret_path)"
				],
				"execution_count": 116
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}
