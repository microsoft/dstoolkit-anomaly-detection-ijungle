{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "\"conf\": {\r\n",
        "    \"spark.sql.autoBroadcastJoinThreshold\": -1,\r\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\r\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 8\r\n",
        "   }\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.ml.functions import vector_to_array\r\n",
        "from pyspark.sql.functions import udf, struct\r\n",
        "from pyspark.sql.types import FloatType\r\n",
        "import numpy as np\r\n",
        "from io import BytesIO\r\n",
        "import joblib\r\n",
        "import pandas as pd\r\n",
        "import h2o\r\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\r\n",
        "prepped_data_path = ''\r\n",
        "iFor_data_prefix = ''\r\n",
        "overhead_data_path = ''\r\n",
        "overhead_results_prefix = ''\r\n",
        "subsample_list = ''\r\n",
        "trees_list = ''\r\n",
        "train_size = ''\r\n",
        "id_feat = ''\r\n",
        "id_feat_types = ''\r\n",
        "seed = ''\r\n",
        "overhead_size = ''\r\n",
        "time_slice_folder = ''\r\n",
        "extension_level = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\r\n",
        "import logging\r\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\r\n",
        "from opencensus.trace import config_integration\r\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\r\n",
        "from opencensus.trace.tracer import Tracer\r\n",
        "\r\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\r\n",
        "config_integration.trace_integrations(['logging'])\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\r\n",
        "logger.setLevel(logging.INFO)\r\n",
        "\r\n",
        "tracer = Tracer(\r\n",
        "    exporter=AzureExporter(\r\n",
        "        connection_string=instrumentation_connection_string\r\n",
        "    ),\r\n",
        "    sampler=AlwaysOnSampler()\r\n",
        ")\r\n",
        "\r\n",
        "# Spool parameters\r\n",
        "run_time_parameters = {'custom_dimensions': {\r\n",
        "    'batch_id': batch_id,\r\n",
        "    'prepped_data_path': prepped_data_path,\r\n",
        "    'iFor_data_prefix': iFor_data_prefix,\r\n",
        "    'overhead_data_path': overhead_data_path,\r\n",
        "    'overhead_results_prefix': overhead_results_prefix,\r\n",
        "    'subsample_list': subsample_list,\r\n",
        "    'trees_list': trees_list,\r\n",
        "    'train_size': train_size,\r\n",
        "    'id_feat': id_feat,\r\n",
        "    'id_feat_types': id_feat_types,\r\n",
        "    'seed': seed,\r\n",
        "    'overhead_size': overhead_size,\r\n",
        "    'time_slice_folder': time_slice_folder,\r\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\r\n",
        "} }\r\n",
        "  \r\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if prepped_data_path != \"\":\r\n",
        "    prepped_data_path = \"/\".join(prepped_data_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + prepped_data_path.split(\"/\")[-1]\r\n",
        "    logger.info(f'prepped_data_path = {prepped_data_path}')\r\n",
        "if iFor_data_prefix != \"\":\r\n",
        "    iFor_data_prefix = \"/\".join(iFor_data_prefix.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + iFor_data_prefix.split(\"/\")[-1]\r\n",
        "    logger.info(f'iFor_data_prefix = {iFor_data_prefix}')\r\n",
        "if overhead_data_path != \"\":\r\n",
        "    overhead_data_path = \"/\".join(overhead_data_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + overhead_data_path.split(\"/\")[-1]\r\n",
        "    logger.info(f'overhead_data_path = {overhead_data_path}')\r\n",
        "if overhead_results_prefix != \"\":\r\n",
        "    overhead_results_prefix = \"/\".join(overhead_results_prefix.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + overhead_results_prefix.split(\"/\")[-1]\r\n",
        "    logger.info(f'overhead_results_prefix = {overhead_results_prefix}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Casting parameters\r\n",
        "\r\n",
        "#subsample_list = eval(subsample_list)\r\n",
        "subsample_list = [int(i) for i in subsample_list.split(\",\")]\r\n",
        "#trees_list = eval(trees_list)\r\n",
        "trees_list = [int(i) for i in trees_list.split(\",\")]\r\n",
        "train_size = float(train_size)\r\n",
        "#id_feat = eval(id_feat)\r\n",
        "id_feat = [i for i in id_feat.split(\",\")]\r\n",
        "#id_feat_types = eval(id_feat_types)\r\n",
        "id_feat_types = [i for i in id_feat_types.split(\",\")]\r\n",
        "seed = int(seed)\r\n",
        "overhead_size = float(overhead_size)\r\n",
        "extension_level = int(extension_level)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df = spark.read.parquet(prepped_data_path)\r\n",
        "m = df.count()\r\n",
        "logger.info(\"Number of records: {:,}\".format(m))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "max_subsample_size = max(subsample_list)\r\n",
        "num_groups = int(np.ceil(m*train_size/max_subsample_size))\r\n",
        "logger.info(f'Num groups: {str(num_groups)}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Creation of overhead sample\r\n",
        "df_W = df.sample(withReplacement=False, fraction=overhead_size)\r\n",
        "df_W.write.mode('overwrite').parquet(overhead_data_path)\r\n",
        "\r\n",
        "num_feats = len(df_W.head(1)[0]['scaled'])\r\n",
        "df_W_unassembled = df_W.withColumn('f', vector_to_array(\"scaled\")).select(id_feat + [F.col(\"f\")[i] for i in range(num_feats)])\r\n",
        "logger.info(\"Number of records of overhead dataset: {:,}\".format(df_W.count()))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def clf_predict(clf, ex_level, time_slice, group_num, tree_size, subsample_size):\r\n",
        "    def _fun(iterator):\r\n",
        "        if ex_level >= 0:\r\n",
        "            model_filename = \"/tmp/ijungle_{}_{}_{}_{}\".format(time_slice, group_num, tree_size, subsample_size)\r\n",
        "            with open(model_filename, 'wb') as model_file:\r\n",
        "                model_file.write(clf)\r\n",
        "            h2o.init()\r\n",
        "            saved_model = h2o.load_model(model_filename)\r\n",
        "            os.remove(model_filename)\r\n",
        "        for pdf in iterator:\r\n",
        "            pdf.set_index([\"issuer_id_indexed\",\"issued_date\",\"tree_size\",\"subsample_size\",\"group_num\"], inplace=True)\r\n",
        "            if ex_level < 0:\r\n",
        "                _predict = clf.score_samples(pdf)\r\n",
        "            else:\r\n",
        "                #Factor of -1 to align with sklearn formalism\r\n",
        "                hf = h2o.H2OFrame(pdf)\r\n",
        "                _predict = saved_model.predict(hf)\r\n",
        "                _predict = -1.0*_predict['anomaly_score'].as_data_frame().to_numpy().reshape(-1)\r\n",
        "            pdf.reset_index(drop=False, inplace=True)\r\n",
        "            pdf_out = pdf[[\"issuer_id_indexed\",\"issued_date\",\"tree_size\",\"subsample_size\",\"group_num\"]]\r\n",
        "            pdf_out[\"predict\"] = _predict\r\n",
        "            yield(pdf_out)\r\n",
        "    return(_fun)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_iFor = spark.read.parquet(iFor_data_prefix) #add select by partition\r\n",
        "schema = \"issuer_id_indexed integer, issued_date timestamp, tree_size integer, subsample_size integer, group_num integer, predict float\"\r\n",
        "df_predict_exists = False\r\n",
        "for trees in trees_list:\r\n",
        "    for subsample_size in subsample_list:\r\n",
        "        for group_num in range(num_groups):\r\n",
        "            #try:\r\n",
        "            if extension_level < 0:\r\n",
        "                model_bytes = df_iFor.where((F.col('id')==group_num) & (F.col('tree_size')==trees) & (F.col('subsample_size')==subsample_size)).select('model').collect()[0]['model']\r\n",
        "                clf = joblib.load(BytesIO(model_bytes))\r\n",
        "            else:\r\n",
        "                model_bytes = df_iFor.where((F.col('id')==group_num) & (F.col('tree_size')==trees) & (F.col('subsample_size')==subsample_size)).select('model').collect()[0]['model']\r\n",
        "                clf = model_bytes\r\n",
        "            if not df_predict_exists:\r\n",
        "                df_predict_exists = True\r\n",
        "                df_predict = df_W_unassembled.withColumn(\"tree_size\",F.lit(trees)).withColumn(\"subsample_size\",F.lit(subsample_size)).withColumn(\"group_num\",F.lit(group_num)).mapInPandas(clf_predict(clf, extension_level, time_slice_folder, group_num, trees, subsample_size), schema = schema)\r\n",
        "            else:\r\n",
        "                df_predict = df_predict.union(df_W_unassembled.withColumn(\"tree_size\",F.lit(trees)).withColumn(\"subsample_size\",F.lit(subsample_size)).withColumn(\"group_num\",F.lit(group_num)).mapInPandas(clf_predict(clf, extension_level, time_slice_folder, group_num, trees, subsample_size), schema = schema))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_predict.write.mode('overwrite').parquet(overhead_results_prefix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
