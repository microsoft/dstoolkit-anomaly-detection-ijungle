{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "    \"spark.sql.autoBroadcastJoinThreshold\": -1,\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "    \"spark.dynamicAllocation.enabled\": true,\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, FloatType, StringType, BooleanType\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, StandardScalerModel, StringIndexerModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\n",
        "transformed_data_path = ''\n",
        "prepped_data_path = ''\n",
        "features_path = ''\n",
        "model_path = ''\n",
        "id_feat = ''\n",
        "date_feat = ''\n",
        "first_year = ''\n",
        "allowed_null_pct = ''\n",
        "training = ''\n",
        "time_slice_folder = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_id': batch_id,\n",
        "    'transformed_data_path': transformed_data_path,\n",
        "    'prepped_data_path': prepped_data_path,\n",
        "    'features_path': features_path,\n",
        "    'model_path': model_path,\n",
        "    'id_feat': id_feat,\n",
        "    'date_feat': date_feat,\n",
        "    'first_year': first_year,\n",
        "    'allowed_null_pct': allowed_null_pct,\n",
        "    'training': training,\n",
        "    'time_slice_folder': time_slice_folder,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if transformed_data_path != \"\":\n",
        "    transformed_data_path = transformed_data_path + \"/\" + time_slice_folder\n",
        "    logger.info(f'transformed_data_path = {transformed_data_path}')\n",
        "if prepped_data_path != \"\":\n",
        "    prepped_data_path = \"/\".join(prepped_data_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + prepped_data_path.split(\"/\")[-1]\n",
        "    logger.info(f'prepped_data_path = {prepped_data_path}')\n",
        "if features_path != \"\":\n",
        "    features_path = \"/\".join(features_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + features_path.split(\"/\")[-1]\n",
        "    logger.info(f'features_path = {features_path}')\n",
        "if model_path != \"\":\n",
        "    model_path = model_path + \"/\" + time_slice_folder\n",
        "    logger.info(f'model_path = {model_path}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Casting parameters\n",
        "#id_feat = eval(id_feat)\n",
        "id_feat = [i for i in id_feat.split(\",\")]\n",
        "first_year = int(first_year)\n",
        "allowed_null_pct = float(allowed_null_pct)\n",
        "training = eval(training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "schema = StructType([\n",
        "    StructField('issuer_id', StringType(), False),\n",
        "    StructField('issued_date', DateType(), False),\n",
        "    StructField('number_of_transactions', IntegerType(), False),\n",
        "    StructField('total_buyers', IntegerType(), False),\n",
        "    StructField('total_credit_notes',FloatType(), False),\n",
        "    StructField('total_debit_notes',FloatType(), False),\n",
        "    StructField('total_invoices',FloatType(), False),\n",
        "    StructField('total_purchase_invoices',FloatType(), False),\n",
        "    StructField('total_export_invoices',FloatType(), False),\n",
        "    StructField('number_of_transactions_to_self',FloatType(), False),\n",
        "    StructField('total_voucher_to_self',FloatType(), False),\n",
        "    StructField('total_taxable_services',FloatType(), False),\n",
        "    StructField('total_non_taxable_services',FloatType(), False),\n",
        "    StructField('total_taxable_goods',FloatType(), False),\n",
        "    StructField('total_non_taxable_goods',FloatType(), False),\n",
        "    StructField('total_taxable',FloatType(), False),\n",
        "    StructField('total_non_taxable',FloatType(), False),\n",
        "    StructField('total_sales',FloatType(), False),\n",
        "    StructField('total_discounts',FloatType(), False),\n",
        "    StructField('total_voucher',FloatType(), False),\n",
        "    StructField('total_tax',FloatType(), False),\n",
        "    StructField('number_of_purchases',IntegerType(), False),\n",
        "    StructField('total_suppliers',FloatType(), False),\n",
        "    StructField('total_purchases',FloatType(), False),\n",
        "    StructField('pagerank_score',FloatType(), False),\n",
        "    StructField('taxpayer_type',StringType(), False),\n",
        "    StructField('taxpayer_size',StringType(), False),\n",
        "    StructField('main_activity',StringType(), False),\n",
        "    StructField('sec1_activity',StringType(), False),\n",
        "    StructField('sec2_activity',StringType(), False),\n",
        "    StructField('employees_number',StringType(), False), #IntegerType()\n",
        "    StructField('legal_reg_date',StringType(), False),  #DateType()\n",
        "    StructField('tax_reg_date',StringType(), False), #DateType()\n",
        "    StructField('e_inv_enroll_date',StringType(), False), #DateType()\n",
        "    StructField('total_capital',StringType(), False), #FloatType()\n",
        "    StructField('reported_assets',StringType(), False), #BooleanType()\n",
        "    StructField('social_capital',StringType(), False), #FloatType()\n",
        "    StructField('total_assets',StringType(), False), #FloatType()\n",
        "    StructField('total_fixed_assets',StringType(), False), #FloatType()\n",
        "    StructField('total_liabilities',StringType(), False), #FloatType()\n",
        "    StructField('gross_income',StringType(), False), #FloatType()\n",
        "    StructField('net_income',StringType(), False), #FloatType()\n",
        "    StructField('total_vat_sales',StringType(), False), #FloatType()\n",
        "    StructField('credited_einvoicing_value',StringType(), False), #FloatType()\n",
        "    StructField('state',StringType(), False),\n",
        "    StructField('municipality',StringType(), False),\n",
        "    StructField('city',StringType(), False),\n",
        "    StructField('ratio_sales_purchases',FloatType(), False),\n",
        "    StructField('ratio_tax_sales',FloatType(), False),\n",
        "    StructField('ratio_sales_employees',FloatType(), False),\n",
        "    StructField('ratio_buyers_suppliers',FloatType(), False),\n",
        "    StructField('ratio_in_out',FloatType(), False),\n",
        "    StructField('act01',FloatType(), False),\n",
        "    StructField('total_voucher_act01',FloatType(), False),\n",
        "    StructField('act02',FloatType(), False),\n",
        "    StructField('total_voucher_act02',FloatType(), False),\n",
        "    StructField('act03',FloatType(), False),\n",
        "    StructField('total_voucher_act03',FloatType(), False),\n",
        "    StructField('act04',FloatType(), False),\n",
        "    StructField('total_voucher_act04',FloatType(), False),\n",
        "    StructField('act05',FloatType(), False),\n",
        "    StructField('total_voucher_act05',FloatType(), False),\n",
        "    StructField('min_distance_from_supplier',IntegerType(), False),\n",
        "    StructField('min_distance_from_customer',IntegerType(), False),\n",
        "    StructField('min_depth_of_supply_chain',IntegerType(), False),\n",
        "    StructField('min_place_in_supply_chain',FloatType(), False),\n",
        "    StructField('max_distance_from_supplier',IntegerType(), False),\n",
        "    StructField('max_distance_from_customer',IntegerType(), False),\n",
        "    StructField('max_depth_of_supply_chain',IntegerType(), False),\n",
        "    StructField('max_place_in_supply_chain',FloatType(), False),\n",
        "    StructField('issuer_id_indexed', IntegerType(), False)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "#Removing schema for now, until upsteam schema is formalized.\n",
        "#df = spark.read.schema(schema).parquet(\n",
        "#Dropping issuer_id because now index of issuer_id is used for id_feat. \n",
        "#May be better to transform the column in here and drop issuer_id_indexed from the transformed datasets.\n",
        "with tracer.span('Loading transformed data'):\n",
        "    df = spark.read.parquet(\n",
        "        transformed_data_path,\n",
        "        header=True\n",
        "    ).drop('issuer_id')\n",
        "    m = df.count()\n",
        "    logger.info(f'Number of records: {m}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for row in schema:\n",
        "    #For now continue until schema is fully stable.\n",
        "    continue\n",
        "    column = row.name\n",
        "    dataType = row.dataType\n",
        "    #exclude dateType for now\n",
        "    if dataType == DateType(): continue\n",
        "    df = df.withColumn(column,F.col(column).cast(dataType))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Removing features with high percentaje of null values\n",
        "allowed_null_feats = []\n",
        "for feat in df.columns:\n",
        "    null_pct = df.where(F.isnull(feat)).count()/m \n",
        "    if null_pct <= allowed_null_pct:\n",
        "        allowed_null_feats.append(feat)\n",
        "    else:\n",
        "        logger.info(f'Feature {feat} has {null_pct*100:.2f}% of null values')\n",
        "\n",
        "df_allowed_null = df.select(allowed_null_feats)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Removing null values\n",
        "df_notnull = df_allowed_null\n",
        "\n",
        "for feat in df_notnull.schema.fieldNames():\n",
        "    df_notnull = df_notnull.where(~F.isnull(feat))\n",
        "\n",
        "logger.info(f'Not null records: {df_notnull.count():,}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Removing records previous to first year parameter\n",
        "df_recent = df_notnull.where(F.year(date_feat) >= first_year)\n",
        "logger.info(f'Number of records since {first_year}: {df_recent.count():,}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Date data augmentation\n",
        "df_augmented = df_recent.withColumn('_dayofweek', F.dayofweek(date_feat))\n",
        "df_augmented = df_augmented.withColumn('_dayofmonth', F.dayofmonth(date_feat))\n",
        "df_augmented = df_augmented.withColumn('_dayofyear', F.dayofyear(date_feat))\n",
        "df_augmented = df_augmented.withColumn('_weekofyear', F.weekofyear(date_feat))\n",
        "df_augmented = df_augmented.withColumn('_month', F.month(date_feat))\n",
        "df_augmented = df_augmented.withColumn('_quarter', F.quarter(date_feat))\n",
        "df_augmented = df_augmented.withColumn('_year', F.year(date_feat))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Date to int\n",
        "date_feats = [x['name'] for x in df_augmented.schema.jsonValue()['fields'] if x['type']=='date']\n",
        "\n",
        "df_date_int = df_augmented\n",
        "\n",
        "for feat in date_feats:\n",
        "    logger.info(f'Casting date feature {feat} to int ...')\n",
        "    df_date_int = df_date_int.withColumn(feat+'_int', F.unix_timestamp(feat))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "string_feats = [x['name'] for x in df_date_int.schema.jsonValue()['fields'] if x['type']=='string']\n",
        "string_feats_indexed = [feat+'_indexed' for feat in string_feats]\n",
        "if training:\n",
        "    indexer = StringIndexer(inputCols=string_feats, outputCols=string_feats_indexed, stringOrderType='frequencyDesc')\n",
        "    model = indexer.fit(df_date_int)\n",
        "    model.write().overwrite().save(model_path + '/' + '_ijungle_indexer.pkl')\n",
        "else:\n",
        "    model = StringIndexerModel.load(model_path + '/' + '_ijungle_indexer.pkl')\n",
        "df_string_indexed = model.transform(df_date_int)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Assemble features to scalate\n",
        "columns = df_string_indexed.schema.fieldNames()\n",
        "feats_to_remove = id_feat + date_feats + string_feats\n",
        "feats = [feat for feat in columns if not feat in feats_to_remove]\n",
        "assembler = VectorAssembler(inputCols=feats, outputCol='feats')\n",
        "df_assembled = assembler.transform(df_string_indexed).select(id_feat + ['feats'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Store features \n",
        "if training:\n",
        "    spark.createDataFrame(zip(range(len(feats)), feats),['id','feat']).write.mode('overwrite').parquet(features_path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Scalate features\n",
        "if training:\n",
        "    scaler = StandardScaler(inputCol='feats', outputCol='scaled')\n",
        "    model = scaler.fit(df_assembled)\n",
        "    model.write().overwrite().save(model_path + '/' + '_ijungle_scaler.pkl')\n",
        "else:\n",
        "    model = StandardScalerModel.load(model_path + '/' + '_ijungle_scaler.pkl')\n",
        "df_scaled = model.transform(df_assembled).select(id_feat+['scaled'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write scaled data as parquet files\n",
        "df_scaled.write.mode('overwrite').parquet(prepped_data_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
