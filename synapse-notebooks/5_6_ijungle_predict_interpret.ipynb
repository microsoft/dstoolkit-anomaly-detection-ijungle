{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "    \"spark.sql.autoBroadcastJoinThreshold\": -1,\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "    \"spark.dynamicAllocation.enabled\": true,\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.ml.feature import IndexToString, StringIndexerModel\n",
        "from pyspark.sql.types import FloatType\n",
        "import joblib\n",
        "import numpy as np\n",
        "import shap\n",
        "from io import BytesIO\n",
        "import pandas as pd\n",
        "import h2o"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\n",
        "best_iforest_path = ''\n",
        "prepped_data_path = ''\n",
        "results_path = ''\n",
        "features_path = ''\n",
        "interpret_path = ''\n",
        "id_feat = ''\n",
        "id_feat_types = ''\n",
        "score_threshold = ''\n",
        "time_slice_folder = ''\n",
        "number_of_interpret_features = ''\n",
        "extension_level = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_id': batch_id,\n",
        "    'best_iforest_path': best_iforest_path,\n",
        "    'prepped_data_path': prepped_data_path,\n",
        "    'results_path': results_path,\n",
        "    'features_path': features_path,\n",
        "    'interpret_path': interpret_path,\n",
        "    'id_feat': id_feat,\n",
        "    'id_feat_types': id_feat_types,\n",
        "    'score_threshold': score_threshold,\n",
        "    'time_slice_folder': time_slice_folder,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if best_iforest_path != \"\":\n",
        "    best_iforest_path = \"/\".join(best_iforest_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + best_iforest_path.split(\"/\")[-1]\n",
        "    logger.info(f'best_iforest_path = {best_iforest_path}')\n",
        "if prepped_data_path != \"\":\n",
        "    prepped_data_path = \"/\".join(prepped_data_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + prepped_data_path.split(\"/\")[-1]\n",
        "    logger.info(f'prepped_data_path = {prepped_data_path}')\n",
        "if results_path != \"\":\n",
        "    results_path = results_path + \"/\" + time_slice_folder\n",
        "    logger.info(f'results_path = {results_path}')\n",
        "if features_path != \"\":\n",
        "    features_path = \"/\".join(features_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + features_path.split(\"/\")[-1]\n",
        "    logger.info(f'features_path = {features_path}')\n",
        "if interpret_path != \"\":\n",
        "    interpret_path = interpret_path + \"/\" + time_slice_folder\n",
        "    logger.info(f'interpret_path = {interpret_path}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Cast parameters\n",
        "id_feat = [i for i in id_feat.split(\",\")]\n",
        "id_feat_types = [i for i in id_feat_types.split(\",\")]\n",
        "score_threshold = float(score_threshold)\n",
        "number_of_interpret_features = int(number_of_interpret_features)\n",
        "extension_level = int(extension_level)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_results = spark.read.parquet(results_path).where(F.col('score')<=score_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_assembled = df_results.join(spark.read.parquet(prepped_data_path), on=id_feat)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "num_feats = len(df_assembled.head(1)[0]['scaled'])\n",
        "df_unassembled = df_assembled.withColumn('f', vector_to_array(\"scaled\")).select(id_feat + ['score'] + [F.col(\"f\")[i] for i in range(num_feats)])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if extension_level < 0:\n",
        "    model_bytes = spark.read.parquet(best_iforest_path).head(1)[0]['model']\n",
        "    clf = joblib.load(BytesIO(model_bytes))\n",
        "else:\n",
        "    clf = None"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "feats = np.array([row['feat'] for row in spark.read.parquet(features_path).orderBy('id').collect()])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "top_n_feat_lst = ['feat'+str(i+1) for i in range(number_of_interpret_features)]\n",
        "top_n_score_lst = ['score'+str(i+1) for i in range(number_of_interpret_features)]\n",
        "feat_score_lst = list(np.array(list(zip(top_n_feat_lst, top_n_score_lst))).reshape(len(top_n_feat_lst)*2))\n",
        "dcc_str = \", \".join([x[0]+\" \"+x[1] for x in zip(id_feat, id_feat_types)]) + \", score float, \"\n",
        "dcc_str += \", \".join([x[0]+\" \"+x[1] for x in zip(feat_score_lst + ['sum_other_features'], ['string', 'float']*len(top_n_feat_lst) + ['float'])])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def ijungle_interpret(id_feat, clf, feats, top_n_feat_lst, top_n_score_lst, feat_score_lst, ex_level):\n",
        "    def _fun(iterator):\n",
        "        if ex_level < 0:\n",
        "            explainer = shap.TreeExplainer(clf)\n",
        "        for pdf in iterator:\n",
        "            pdf.set_index(id_feat + ['score'], inplace=True)\n",
        "            if ex_level < 0:\n",
        "                shap_values = explainer.shap_values(pdf)\n",
        "            else:\n",
        "                #Dummy variables so PowerBI doesn't break\n",
        "                shap_values = np.array([[1./len(feats) for _ in feats] for _ in range(pdf.shape[0])])\n",
        "            top_feats = feats[shap_values.argsort()[:,:len(top_n_feat_lst)]]\n",
        "            pdf_out = pdf.reset_index()\n",
        "            pdf_out = pdf_out[id_feat + ['score']]\n",
        "            norm_fracs = np.array(pdf_out['score'])/np.sum(shap_values,axis=1)\n",
        "            top_scores = np.sort(shap_values)[:,:len(top_n_feat_lst)]*norm_fracs.reshape(norm_fracs.shape[0],-1)\n",
        "            pdf_out[top_n_feat_lst] = top_feats\n",
        "            pdf_out[top_n_score_lst] = top_scores\n",
        "            pdf_out['sum_other_features'] = np.sum(np.sort(shap_values)[:,len(top_n_feat_lst):],axis=1)*norm_fracs\n",
        "            pdf_out = pdf_out[id_feat + ['score'] + feat_score_lst + ['sum_other_features']]\n",
        "            yield(pdf_out)\n",
        "    return(_fun)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_interpret = df_unassembled.mapInPandas(ijungle_interpret(id_feat, clf, feats, top_n_feat_lst, top_n_score_lst, feat_score_lst, extension_level), dcc_str)\r\n",
        "model = StringIndexerModel.load(\"/\".join(best_iforest_path.split(\"/\")[:-2]) + '/' + '_feature_engineering_indexer_issuer_id.pkl')\r\n",
        "inverter = IndexToString(inputCol=\"issuer_id_indexed\", outputCol=\"issuer_id\", labels=model.labels)\r\n",
        "df_interpret = inverter.transform(df_interpret)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Saving interpret data to ADLS'):\r\n",
        "    df_interpret.write.mode('overwrite').parquet(interpret_path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "new_rows = ['issuer_id','issued_date']\r\n",
        "new_rows += [F.array(F.array(F.lit('total_score'),'score'),*[F.array(feat, score) for idx, (feat, score) in enumerate(zip(top_n_feat_lst, top_n_score_lst))],F.array(F.lit('sum_other_features'),'sum_other_features')).alias(\"merged_rows\")]\r\n",
        "df_interpret_by_row = df_interpret.select(*new_rows)\r\n",
        "df_interpret_by_row = df_interpret_by_row.select('issuer_id','issued_date',F.explode('merged_rows').alias('key_value'))\r\n",
        "df_interpret_by_row = df_interpret_by_row.withColumn('feature',df_interpret_by_row['key_value'].getItem(0)).withColumn('score',df_interpret_by_row['key_value'].getItem(1).cast(FloatType())).drop('key_value')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "interpret_path_by_row = '/'.join(interpret_path.split(\"/\")[:-1]) + \"_exploded\" + \"/\" + interpret_path.split(\"/\")[-1]\r\n",
        "logger.info(interpret_path)\r\n",
        "logger.info(interpret_path_by_row)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Saving interpret by row data to ADLS'):\r\n",
        "    df_interpret_by_row.write.mode('overwrite').parquet(interpret_path_by_row)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# serverless SQL config\n",
        "import pyodbc\n",
        "database = 'eiad'\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\n",
        "\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SyanpseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def generate_schema_string(dataframe):\n",
        "    schema_string = \"\"\n",
        "    for name in dataframe.schema.fieldNames():\n",
        "        schema_string += \"[\" + name + \"] \"\n",
        "        datatype = str(dataframe.schema[name].dataType.simpleString())\n",
        "        if datatype == 'double': datatype = 'float'\n",
        "        if datatype == 'string': datatype = 'nvarchar(MAX)'\n",
        "        if datatype == 'timestamp': datatype = 'datetime2(7)'\n",
        "        schema_string += datatype + \", \"\n",
        "    return schema_string[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Creating SQL table for interpret data'):\n",
        "    table_name = interpret_path.split('/')[3] + '_' + interpret_path.split('/')[2].split('@')[0] + '_' + interpret_path.split('/')[4] + '_' + interpret_path.split('/')[5]\n",
        "    schema_string = generate_schema_string(df_interpret)\n",
        "    drop_table_command = f\"DROP EXTERNAL TABLE [{table_name}]\"\n",
        "    location = \"/\".join([i for idx, i in enumerate(interpret_path.split('/')) if idx > 2])\n",
        "    df_sql_command = f\"CREATE EXTERNAL TABLE [{table_name}] ({schema_string}) WITH (LOCATION = '{location}/**', DATA_SOURCE = [output_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            try:\n",
        "                cursor.execute(drop_table_command)\n",
        "            except:\n",
        "                pass\n",
        "            cursor.execute(df_sql_command)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Creating SQL table for interpret by row data'):\r\n",
        "    table_name = interpret_path_by_row.split('/')[3] + '_' + interpret_path_by_row.split('/')[2].split('@')[0] + '_' + interpret_path_by_row.split('/')[4] + '_' + interpret_path_by_row.split('/')[5]\r\n",
        "    schema_string = generate_schema_string(df_interpret_by_row)\r\n",
        "    drop_table_command = f\"DROP EXTERNAL TABLE [{table_name}]\"\r\n",
        "    location = \"/\".join([i for idx, i in enumerate(interpret_path_by_row.split('/')) if idx > 2])\r\n",
        "    df_sql_command = f\"CREATE EXTERNAL TABLE [{table_name}] ({schema_string}) WITH (LOCATION = '{location}/**', DATA_SOURCE = [output_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\r\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            try:\r\n",
        "                cursor.execute(drop_table_command)\r\n",
        "            except:\r\n",
        "                pass\r\n",
        "            cursor.execute(df_sql_command)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
