{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "\"conf\": {\r\n",
        "    \"spark.sql.autoBroadcastJoinThreshold\": -1,\r\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\r\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 8\r\n",
        "   }\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\r\n",
        "iFor_data_prefix = ''\r\n",
        "overhead_data_path = ''\r\n",
        "overhead_results_prefix = ''\r\n",
        "best_iforest_path = ''\r\n",
        "subsample_list = ''\r\n",
        "trees_list = ''\r\n",
        "id_feat = ''\r\n",
        "time_slice_folder = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\r\n",
        "import logging\r\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\r\n",
        "from opencensus.trace import config_integration\r\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\r\n",
        "from opencensus.trace.tracer import Tracer\r\n",
        "\r\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\r\n",
        "config_integration.trace_integrations(['logging'])\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\r\n",
        "logger.setLevel(logging.INFO)\r\n",
        "\r\n",
        "tracer = Tracer(\r\n",
        "    exporter=AzureExporter(\r\n",
        "        connection_string=instrumentation_connection_string\r\n",
        "    ),\r\n",
        "    sampler=AlwaysOnSampler()\r\n",
        ")\r\n",
        "\r\n",
        "# Spool parameters\r\n",
        "run_time_parameters = {'custom_dimensions': {\r\n",
        "    'batch_id': batch_id,\r\n",
        "    'iFor_data_prefix': iFor_data_prefix,\r\n",
        "    'overhead_data_path': overhead_data_path,\r\n",
        "    'overhead_results_prefix': overhead_results_prefix,\r\n",
        "    'best_iforest_path': best_iforest_path,\r\n",
        "    'subsample_list': subsample_list,\r\n",
        "    'trees_list': trees_list,\r\n",
        "    'id_feat': id_feat,\r\n",
        "    'time_slice_folder': time_slice_folder,\r\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\r\n",
        "} }\r\n",
        "  \r\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if iFor_data_prefix != \"\":\r\n",
        "    iFor_data_prefix = \"/\".join(iFor_data_prefix.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + iFor_data_prefix.split(\"/\")[-1]\r\n",
        "    logger.info(f'iFor_data_prefix = {iFor_data_prefix}')\r\n",
        "if overhead_data_path != \"\":\r\n",
        "    overhead_data_path = \"/\".join(overhead_data_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + overhead_data_path.split(\"/\")[-1]\r\n",
        "    logger.info(f'overhead_data_path = {overhead_data_path}')\r\n",
        "if overhead_results_prefix != \"\":\r\n",
        "    overhead_results_prefix = \"/\".join(overhead_results_prefix.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + overhead_results_prefix.split(\"/\")[-1]\r\n",
        "    logger.info(f'overhead_results_prefix = {overhead_results_prefix}')\r\n",
        "if best_iforest_path != \"\":\r\n",
        "    best_iforest_path = \"/\".join(best_iforest_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + best_iforest_path.split(\"/\")[-1]\r\n",
        "    logger.info(f'best_iforest_path = {best_iforest_path}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Casting parameters\r\n",
        "subsample_list = [int(i) for i in subsample_list.split(\",\")]\r\n",
        "trees_list = [int(i) for i in trees_list.split(\",\")]\r\n",
        "id_feat = [i for i in id_feat.split(\",\")]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_predict = spark.read.parquet(overhead_results_prefix)\r\n",
        "df_avg = df_predict.groupBy(id_feat).agg((F.sum('predict')/F.count('predict')).alias('avg'))\r\n",
        "df_predict = df_predict.join(df_avg,on=id_feat)\r\n",
        "df_predict = df_predict.withColumn(\"squared_residuals\",F.pow(F.col('predict') - F.col(\"avg\"),2))\r\n",
        "df_model = df_predict.groupBy(\"tree_size\",\"subsample_size\",\"group_num\").agg(F.sum(\"squared_residuals\").alias(\"sum_of_squared_residuals\"))\r\n",
        "df_model = df_model.orderBy(\"sum_of_squared_residuals\",ascending=True)\r\n",
        "best_trees, best_subsample_size, best_group, _ = df_model.head(1)[0]\r\n",
        "logger.info(\"Best iForest: {}, {}, {}\".format(best_trees, best_subsample_size, best_group))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_iFor = spark.read.parquet(iFor_data_prefix)\r\n",
        "model_bytes = df_iFor.where((F.col('id')==best_group)&(F.col('tree_size')==best_trees)&(F.col('subsample_size')==best_subsample_size)).select('model').collect()[0]['model']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.createDataFrame([('best_iforest',model_bytes)],schema=['id','model']).write.mode('overwrite').parquet(best_iforest_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
