{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "\"conf\": {\r\n",
        "    \"spark.sql.autoBroadcastJoinThreshold\": -1,\r\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\r\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 8\r\n",
        "   }\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.window import Window\r\n",
        "from pyspark.ml.functions import vector_to_array\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.ensemble import IsolationForest\r\n",
        "import joblib\r\n",
        "from io import BytesIO\r\n",
        "import h2o\r\n",
        "from h2o.estimators.extended_isolation_forest import H2OExtendedIsolationForestEstimator as ExtendedIsolationForest\r\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\r\n",
        "prepped_data_path = ''\r\n",
        "iFor_data_prefix = ''\r\n",
        "subsample_list = ''\r\n",
        "trees_list = ''\r\n",
        "train_size = ''\r\n",
        "id_feat = ''\r\n",
        "seed = ''\r\n",
        "time_slice_folder = ''\r\n",
        "extension_level = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\r\n",
        "import logging\r\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\r\n",
        "from opencensus.trace import config_integration\r\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\r\n",
        "from opencensus.trace.tracer import Tracer\r\n",
        "\r\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\r\n",
        "config_integration.trace_integrations(['logging'])\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\r\n",
        "logger.setLevel(logging.INFO)\r\n",
        "\r\n",
        "tracer = Tracer(\r\n",
        "    exporter=AzureExporter(\r\n",
        "        connection_string=instrumentation_connection_string\r\n",
        "    ),\r\n",
        "    sampler=AlwaysOnSampler()\r\n",
        ")\r\n",
        "\r\n",
        "# Spool parameters\r\n",
        "run_time_parameters = {'custom_dimensions': {\r\n",
        "    'batch_id': batch_id,\r\n",
        "    'prepped_data_path': prepped_data_path,\r\n",
        "    'iFor_data_prefix': iFor_data_prefix,\r\n",
        "    'subsample_list': subsample_list,\r\n",
        "    'trees_list': trees_list,\r\n",
        "    'train_size': train_size,\r\n",
        "    'id_feat': id_feat,\r\n",
        "    'seed': seed,\r\n",
        "    'time_slice_folder': time_slice_folder,\r\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\r\n",
        "} }\r\n",
        "  \r\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if prepped_data_path != \"\":\r\n",
        "    prepped_data_path = \"/\".join(prepped_data_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + prepped_data_path.split(\"/\")[-1]\r\n",
        "    logger.info(f'prepped_data_path = {prepped_data_path}')\r\n",
        "if iFor_data_prefix != \"\":\r\n",
        "    iFor_data_prefix = \"/\".join(iFor_data_prefix.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + iFor_data_prefix.split(\"/\")[-1]\r\n",
        "    logger.info(f'iFor_data_prefix = {iFor_data_prefix}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Cast parameters\r\n",
        "subsample_list = [int(i) for i in subsample_list.split(\",\")]\r\n",
        "trees_list = [int(i) for i in trees_list.split(\",\")]\r\n",
        "train_size = float(train_size)\r\n",
        "id_feat = [i for i in id_feat.split(\",\")]\r\n",
        "seed = int(seed)\r\n",
        "extension_level = int(extension_level)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "max_subsample_size = max(subsample_list)\r\n",
        "w = Window().orderBy(F.lit('A'))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df = spark.read.parquet(prepped_data_path)\r\n",
        "m = df.count()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "num_groups = int(np.ceil(m*train_size/max_subsample_size))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Add id to join with group table\r\n",
        "df_id = df.withColumn('_id',F.row_number().over(w))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def ijungle_train(id_feat, seed, time_slice, ex_level):\r\n",
        "    def _fun(key, pdf):\r\n",
        "        trees = key[0]\r\n",
        "        subsample_size = key[1]\r\n",
        "        group = key[2]\r\n",
        "        pdf.set_index(id_feat, inplace=True)\r\n",
        "        feats = list(pdf.columns)\r\n",
        "        feats.remove('_group')\r\n",
        "        feats.remove('_tree_size')\r\n",
        "        feats.remove('_subsample_size')\r\n",
        "        pdf = pdf[feats]\r\n",
        "        if ex_level < 0:\r\n",
        "            clf = IsolationForest(\r\n",
        "                n_estimators = trees, \r\n",
        "                max_samples=min(subsample_size, pdf.shape[0]), \r\n",
        "                random_state=seed, n_jobs=-1)\r\n",
        "            clf.fit(pdf)\r\n",
        "            bytes_container = BytesIO()\r\n",
        "            joblib.dump(clf, bytes_container)\r\n",
        "            bytes_container.seek(0)\r\n",
        "            model_bytes = bytes_container.read()\r\n",
        "            return(pd.DataFrame([(trees, subsample_size, group, model_bytes)]))\r\n",
        "        else:\r\n",
        "            h2o.init()\r\n",
        "            clf = ExtendedIsolationForest(\r\n",
        "                ntrees= int(trees), \r\n",
        "                sample_size=int(min(subsample_size, pdf.shape[0])), \r\n",
        "                seed=int(seed), \r\n",
        "                extension_level=int(ex_level))\r\n",
        "            hf = h2o.H2OFrame(pdf)\r\n",
        "            clf.train(training_frame=hf)\r\n",
        "            model_path = \"/tmp/ijungle_{}_{}_{}_{}\".format(time_slice,trees,subsample_size,group)\r\n",
        "            model_filename = h2o.save_model(model=clf, path=model_path, force=True)\r\n",
        "            with open(model_filename, 'rb') as model_file:\r\n",
        "                file_bytes = model_file.read()\r\n",
        "            shutil.rmtree(model_path)\r\n",
        "            return(pd.DataFrame([(trees, subsample_size, group, file_bytes)]))\r\n",
        "    return(_fun)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_unassembled_exists = False\r\n",
        "for trees in trees_list:\r\n",
        "    for subsample_size in subsample_list:\r\n",
        "        # Random selection of records in groups of subsample size\r\n",
        "        group_array = np.array([])\r\n",
        "        for group in range(num_groups):\r\n",
        "            group_array = np.concatenate([group_array, group * np.ones(subsample_size)])\r\n",
        "\r\n",
        "        group_array = np.concatenate([group_array, -1*np.ones(m-(num_groups*subsample_size))])\r\n",
        "\r\n",
        "        np.random.shuffle(group_array)\r\n",
        "\r\n",
        "        pdf_id_group = pd.DataFrame(group_array, columns=['_group']).reset_index()\r\n",
        "        pdf_id_group.columns = ['_id', '_group']\r\n",
        "\r\n",
        "        df_id_group = spark.createDataFrame(pdf_id_group)\r\n",
        "\r\n",
        "        # Join of random selection of groups with training data\r\n",
        "        df_subsamples = df_id.join(df_id_group, on='_id').where(F.col('_group')>=0).select(id_feat+['scaled','_group'])\r\n",
        "        df_subsamples = df_subsamples.withColumn(\"_tree_size\",F.lit(trees)).withColumn(\"_subsample_size\",F.lit(subsample_size)) \r\n",
        "        df_subsamples = df_subsamples.cache()\r\n",
        "\r\n",
        "        # Vector to individual columns to prepare for parallel training\r\n",
        "        num_feats = len(df_subsamples.head(1)[0]['scaled'])\r\n",
        "        if df_unassembled_exists:\r\n",
        "            df_unassembled = df_unassembled.union(df_subsamples.withColumn('f', vector_to_array(\"scaled\")).select(id_feat + ['_tree_size','_subsample_size','_group'] + [F.col(\"f\")[i] for i in range(num_feats)]))\r\n",
        "        else:\r\n",
        "            df_unassembled = df_subsamples.withColumn('f', vector_to_array(\"scaled\")).select(id_feat + ['_tree_size','_subsample_size','_group'] + [F.col(\"f\")[i] for i in range(num_feats)])\r\n",
        "            df_unassembled_exists = True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_iFor = df_unassembled.groupBy('_tree_size', '_subsample_size', '_group').applyInPandas(\r\n",
        "    ijungle_train(id_feat, seed, time_slice_folder, extension_level), \r\n",
        "    schema=\"tree_size long, subsample_size long, id long, model binary\"\r\n",
        ")\r\n",
        "df_iFor.write.mode('overwrite').parquet(iFor_data_prefix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
