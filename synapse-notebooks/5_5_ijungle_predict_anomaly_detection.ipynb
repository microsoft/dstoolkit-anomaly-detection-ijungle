{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "    \"spark.sql.autoBroadcastJoinThreshold\": -1,\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "    \"spark.dynamicAllocation.enabled\": true,\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import joblib\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.ml.feature import IndexToString, StringIndexerModel\n",
        "import pyspark.sql.functions as F\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import h2o\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\n",
        "best_iforest_path = ''\n",
        "prepped_data_path = ''\n",
        "results_path = ''\n",
        "id_feat = ''\n",
        "id_feat_types = ''\n",
        "time_slice_folder = ''\n",
        "model_path = ''\n",
        "extension_level = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_id': batch_id,\n",
        "    'best_iforest_path': best_iforest_path,\n",
        "    'prepped_data_path': prepped_data_path,\n",
        "    'results_path': results_path,\n",
        "    'id_feat': id_feat,\n",
        "    'id_feat_types': id_feat_types,\n",
        "    'time_slice_folder': time_slice_folder,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if best_iforest_path != \"\":\n",
        "    best_iforest_path = \"/\".join(best_iforest_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + best_iforest_path.split(\"/\")[-1]\n",
        "    logger.info(f'best_iforest_path = {best_iforest_path}')\n",
        "if prepped_data_path != \"\":\n",
        "    prepped_data_path = \"/\".join(prepped_data_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + prepped_data_path.split(\"/\")[-1]\n",
        "    logger.info(f'prepped_data_path = {prepped_data_path}')\n",
        "if results_path != \"\":\n",
        "    results_path = results_path + \"/\" + time_slice_folder\n",
        "    logger.info(f'results_path = {results_path}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Casting parameters\n",
        "id_feat = [i for i in id_feat.split(\",\")]\n",
        "id_feat_types = [i for i in id_feat_types.split(\",\")]\n",
        "extension_level = int(extension_level)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "model_bytes = spark.read.parquet(best_iforest_path).head(1)[0]['model']\n",
        "if extension_level < 0:\n",
        "    clf = joblib.load(BytesIO(model_bytes))\n",
        "else:\n",
        "    clf = model_bytes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df = spark.read.parquet(prepped_data_path)\n",
        "logger.info(\"Number of records: {:,}\".format(df.count()))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "num_feats = len(df.head(1)[0]['scaled'])\n",
        "logger.info(f'Number of features: {str(num_feats)}')\n",
        "df_unassembled = df.withColumn('f', vector_to_array(\"scaled\")).select(id_feat + [F.col(\"f\")[i] for i in range(num_feats)])\n",
        "logger.info(\"Number of records of inference dataset: {:,}\".format(df_unassembled.count()))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def ijungle_predict(id_feat, clf, ex_level, time_slice):\n",
        "    def _fun(iterator):\n",
        "        if ex_level >= 0:\n",
        "            model_filename = \"/tmp/ijungle_{}\".format(time_slice)\n",
        "            with open(model_filename, 'wb') as model_file:\n",
        "                model_file.write(clf)\n",
        "            h2o.init()\n",
        "            saved_model = h2o.load_model(model_filename)\n",
        "            os.remove(model_filename)\n",
        "        for pdf in iterator:\n",
        "            pdf.set_index(id_feat, inplace=True)\n",
        "            if ex_level < 0:\n",
        "                _predict = clf.predict(pdf)\n",
        "                _score = clf.score_samples(pdf)\n",
        "            else:\n",
        "                #Factor of -1 to align with sklearn formalism\n",
        "                hf = h2o.H2OFrame(pdf)\n",
        "                _score = saved_model.predict(hf)\n",
        "                _score = -1.0*_score['anomaly_score'].as_data_frame().to_numpy().reshape(-1)\n",
        "                _predict = np.where(_score < -0.5, -1, 1)\n",
        "            pdf.reset_index(drop=False, inplace=True)\n",
        "            pdf_out = pd.DataFrame()\n",
        "            pdf_out[id_feat] = pdf[id_feat]\n",
        "            pdf_out['predict'] = _predict\n",
        "            pdf_out['score'] = _score\n",
        "            yield(pdf_out)\n",
        "    return(_fun)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dcc_str = \", \".join([x[0]+\" \"+x[1] for x in zip(id_feat, id_feat_types)]) + \", predict int, score float\"\n",
        "df_results = df_unassembled.mapInPandas(ijungle_predict(id_feat, clf, extension_level, time_slice_folder),dcc_str)\n",
        "model = StringIndexerModel.load(\"/\".join(best_iforest_path.split(\"/\")[:-2]) + '/' + '_feature_engineering_indexer_issuer_id.pkl')\n",
        "inverter = IndexToString(inputCol=\"issuer_id_indexed\", outputCol=\"issuer_id\", labels=model.labels)\n",
        "df_results = inverter.transform(df_results)\n",
        "#May need to add casting to ensure issuer_id is a string\n",
        "#df_results = df_results.withColumn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_results.write.mode('overwrite').parquet(results_path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# serverless SQL config\n",
        "import pyodbc\n",
        "database = 'eiad'\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\n",
        "\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SyanpseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def generate_schema_string(dataframe):\n",
        "    schema_string = \"\"\n",
        "    for name in dataframe.schema.fieldNames():\n",
        "        schema_string += \"[\" + name + \"] \"\n",
        "        datatype = str(dataframe.schema[name].dataType.simpleString())\n",
        "        if datatype == 'double': datatype = 'float'\n",
        "        if datatype == 'string': datatype = 'nvarchar(MAX)'\n",
        "        if datatype == 'timestamp': datatype = 'datetime2(7)'\n",
        "        schema_string += datatype + \", \"\n",
        "    return schema_string[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Creating SQL table for anomaly detection results'):\n",
        "    table_name = results_path.split('/')[3] + '_' + results_path.split('/')[2].split('@')[0] + '_' + results_path.split('/')[4] + '_' + results_path.split('/')[5]\n",
        "    schema_string = generate_schema_string(df_results)\n",
        "    drop_table_command = f\"DROP EXTERNAL TABLE [{table_name}]\"\n",
        "    location = \"/\".join([i for idx, i in enumerate(results_path.split('/')) if idx > 2])\n",
        "    df_sql_command = f\"CREATE EXTERNAL TABLE [{table_name}] ({schema_string}) WITH (LOCATION = '{location}/**', DATA_SOURCE = [output_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            try:\n",
        "                cursor.execute(drop_table_command)\n",
        "            except:\n",
        "                pass\n",
        "            cursor.execute(df_sql_command)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
