{
	"name": "05 - ijungle - predict - anomaly detection",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7b21d62e-eff9-46ea-bc18-1d6088b60e23"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import joblib\r\n",
					"from pyspark.ml.functions import vector_to_array\r\n",
					"import pyspark.sql.functions as F"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"best_iforest_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/06-best_iforest'\r\n",
					"predict_prepped_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/07-Predict_Prepped_Data'\r\n",
					"results_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/08-Result_Data'\r\n",
					"id_feat = \"['issuer_id','issued_date']\"\r\n",
					"id_feat_types = \"['int', 'date']\""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Casting parameters\r\n",
					"\r\n",
					"id_feat = eval(id_feat)\r\n",
					"id_feat_types = eval(id_feat_types)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"model_bytes = spark.read.parquet(best_iforest_path).head(1)[0]['model']\r\n",
					"with open('best_iforest.pkl', 'bw') as model_file:\r\n",
					"    model_file.write(model_bytes)\r\n",
					"clf = joblib.load('best_iforest.pkl')\r\n",
					"clf"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df = spark.read.parquet(predict_prepped_path)\r\n",
					"print(\"Number of records: {:,}\".format(df.count()))"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"num_feats = len(df.head(1)[0]['scaled'])\r\n",
					"print(\"Number of features:\", num_feats)\r\n",
					"df_unassembled = df.withColumn('f', vector_to_array(\"scaled\")).select(id_feat + [F.col(\"f\")[i] for i in range(num_feats)])\r\n",
					"print(\"Number of records of overhead dataset: {:,}\".format(df_unassembled.count()))"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def ijungle_predict(id_feat, clf):\r\n",
					"    def _fun(iterator):\r\n",
					"        import pandas as pd\r\n",
					"        for pdf in iterator:\r\n",
					"            pdf.set_index(id_feat, inplace=True)\r\n",
					"            _predict = clf.predict(pdf)\r\n",
					"            _score = clf.score_samples(pdf)\r\n",
					"            pdf.reset_index(drop=False, inplace=True)\r\n",
					"            pdf_out = pd.DataFrame()\r\n",
					"            pdf_out[id_feat] = pdf[id_feat]\r\n",
					"            pdf_out['predict'] = _predict\r\n",
					"            pdf_out['score'] = _score\r\n",
					"            yield(pdf_out)\r\n",
					"    return(_fun)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dcc_str = \", \".join([x[0]+\" \"+x[1] for x in zip(id_feat, id_feat_types)]) + \", predict int, score float\"\r\n",
					"df_results = df_unassembled.mapInPandas(ijungle_predict(id_feat, clf),dcc_str)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_results.write.mode('overwrite').parquet(results_path)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}
