{
	"name": "01 - ijungle - common - feature engineering",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "small",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "29868e2b-e68d-4a40-b3e6-abf08d632430"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/024dc50c-b360-41e5-9702-f88c150130de/resourceGroups/rg-guspabon-sbx/providers/Microsoft.Synapse/workspaces/syw-guspabon-001/bigDataPools/small",
				"name": "small",
				"type": "Spark",
				"endpoint": "https://syw-guspabon-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/small",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import StructType, StructField, IntegerType, DateType, FloatType, StringType, BooleanType\r\n",
					"import pyspark.sql.functions as F\r\n",
					"from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, StandardScalerModel, StringIndexerModel"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"transformed_data_path = 'abfss://transformeddata@eiadapstoreworking.dfs.core.windows.net/TransformedData'\r\n",
					"# prepped_data_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/02-Prepped_Data'\r\n",
					"prepped_data_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/07-Predict_Prepped_Data'\r\n",
					"features_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/02-Features'\r\n",
					"id_feat = \"['issuer_id','issued_date']\"\r\n",
					"date_feat = 'issued_date'\r\n",
					"first_year = \"1950\"\r\n",
					"allowed_null_pct = \"0.051\"\r\n",
					"# training=\"True\"\r\n",
					"training=\"False\""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Casting parameters\r\n",
					"\r\n",
					"id_feat = eval(id_feat)\r\n",
					"first_year = int(first_year)\r\n",
					"allowed_null_pct = float(allowed_null_pct)\r\n",
					"training = bool(training)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"schema = StructType([\r\n",
					"    StructField('issuer_id', IntegerType(), False),\r\n",
					"    StructField('issued_date', DateType(), False),\r\n",
					"    StructField('number_of_sales', IntegerType(), False),\r\n",
					"    StructField('count_buyers', IntegerType(), False),\r\n",
					"    StructField('sum_document_type_C',FloatType(), False),\r\n",
					"    StructField('sum_document_type_D',FloatType(), False),\r\n",
					"    StructField('sum_document_type_F',FloatType(), False),\r\n",
					"    StructField('sum_document_type_P',FloatType(), False),\r\n",
					"    StructField('sum_document_type_X',FloatType(), False),\r\n",
					"    StructField('sum_self_transactions',FloatType(), False),\r\n",
					"    StructField('sum_total_voucher_to_self',FloatType(), False),\r\n",
					"    StructField('sum_total_taxable_services',FloatType(), False),\r\n",
					"    StructField('sum_total_non_taxable_services',FloatType(), False),\r\n",
					"    StructField('sum_total_taxable_goods',FloatType(), False),\r\n",
					"    StructField('sum_total_non_taxable_goods',FloatType(), False),\r\n",
					"    StructField('sum_total_taxable',FloatType(), False),\r\n",
					"    StructField('sum_total_non_taxable',FloatType(), False),\r\n",
					"    StructField('sum_total_sales',FloatType(), False),\r\n",
					"    StructField('sum_total_discounts',FloatType(), False),\r\n",
					"    StructField('sum_total_voucher',FloatType(), False),\r\n",
					"    StructField('sum_total_tax',FloatType(), False),\r\n",
					"    StructField('number_of_purchases',IntegerType(), False),\r\n",
					"    StructField('count_suppliers',FloatType(), False),\r\n",
					"    StructField('sum_total_purchases',FloatType(), False),\r\n",
					"    StructField('pagerank_score',FloatType(), False),\r\n",
					"    StructField('taxpayer_type',StringType(), False),\r\n",
					"    StructField('taxpayer_size',StringType(), False),\r\n",
					"    StructField('main_activity',StringType(), False),\r\n",
					"    StructField('sec1_activity',StringType(), False),\r\n",
					"    StructField('sec2_activity',StringType(), False),\r\n",
					"    StructField('employees_number',IntegerType(), False),\r\n",
					"    StructField('legal_reg_date',DateType(), False),\r\n",
					"    StructField('tax_reg_date',DateType(), False),\r\n",
					"    StructField('e_inv_enroll_date',DateType(), False),\r\n",
					"    StructField('total_capital',FloatType(), False),\r\n",
					"    StructField('reported_assets',BooleanType(), False),\r\n",
					"    StructField('social_capital',FloatType(), False),\r\n",
					"    StructField('total_assets',FloatType(), False),\r\n",
					"    StructField('total_fixed_assets',FloatType(), False),\r\n",
					"    StructField('total_liabilities',FloatType(), False),\r\n",
					"    StructField('gross_income',FloatType(), False),\r\n",
					"    StructField('net_income',FloatType(), False),\r\n",
					"    StructField('total_vat_sales',FloatType(), False),\r\n",
					"    StructField('credited_einvoicing_value',FloatType(), False),\r\n",
					"    StructField('state',StringType(), False),\r\n",
					"    StructField('municipality',StringType(), False),\r\n",
					"    StructField('city',StringType(), False),\r\n",
					"    StructField('ratio_sales_purchases',FloatType(), False),\r\n",
					"    StructField('ratio_tax_sales',FloatType(), False),\r\n",
					"    StructField('ratio_sales_employees',FloatType(), False),\r\n",
					"    StructField('ratio_buyers_suppliers',FloatType(), False),\r\n",
					"    StructField('ratio_in_out',FloatType(), False),\r\n",
					"    StructField('act01',FloatType(), False),\r\n",
					"    StructField('total_voucher_act01',FloatType(), False),\r\n",
					"    StructField('act02',FloatType(), False),\r\n",
					"    StructField('total_voucher_act02',FloatType(), False),\r\n",
					"    StructField('act03',FloatType(), False),\r\n",
					"    StructField('total_voucher_act03',FloatType(), False),\r\n",
					"    StructField('act04',FloatType(), False),\r\n",
					"    StructField('total_voucher_act04',FloatType(), False),\r\n",
					"    StructField('act05',FloatType(), False),\r\n",
					"    StructField('total_voucher_act05',FloatType(), False),\r\n",
					"    StructField('depth_s',IntegerType(), False),\r\n",
					"    StructField('depth_r',IntegerType(), False),\r\n",
					"    StructField('min_depth_of_supply_chain',IntegerType(), False),\r\n",
					"    StructField('place_in_supply_chain',FloatType(), False)\r\n",
					"])"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"df = spark.read.schema(schema).csv(\r\n",
					"    path=transformed_data_path,\r\n",
					"    header=True)\r\n",
					"m = df.count()\r\n",
					"print('Number of records {:,}'.format(m))"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Removing features with high percentaje of null values\r\n",
					"\r\n",
					"allowed_null_feats = []\r\n",
					"for feat in df.columns:\r\n",
					"    null_pct = df.where(F.isnull(feat)).count()/m \r\n",
					"    if null_pct <= allowed_null_pct:\r\n",
					"        allowed_null_feats.append(feat)\r\n",
					"    else:\r\n",
					"        print(\"Feature {} has {:.2f}% of null values\".format(feat, null_pct*100))\r\n",
					"\r\n",
					"df_allowed_null = df.select(allowed_null_feats)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Removing null values\r\n",
					"\r\n",
					"df_notnull = df_allowed_null\r\n",
					"\r\n",
					"for feat in df_notnull.schema.fieldNames():\r\n",
					"    df_notnull = df_notnull.where(~F.isnull(feat))\r\n",
					"\r\n",
					"print(\"Not null records: {:,}\".format(df_notnull.count()))"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Removing records previous to first year parameter\r\n",
					"df_recent = df_notnull.where(F.year(date_feat) >= first_year)\r\n",
					"print(\"Number of records since {}: {:,}\".format(first_year, df_recent.count()))"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Date data augmentation\r\n",
					"\r\n",
					"df_augmented = df_recent.withColumn('_dayofweek', F.dayofweek(date_feat))\r\n",
					"df_augmented = df_augmented.withColumn('_dayofmonth', F.dayofmonth(date_feat))\r\n",
					"df_augmented = df_augmented.withColumn('_dayofyear', F.dayofyear(date_feat))\r\n",
					"df_augmented = df_augmented.withColumn('_weekofyear', F.weekofyear(date_feat))\r\n",
					"df_augmented = df_augmented.withColumn('_month', F.month(date_feat))\r\n",
					"df_augmented = df_augmented.withColumn('_quarter', F.quarter(date_feat))\r\n",
					"df_augmented = df_augmented.withColumn('_year', F.year(date_feat))"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Date to int\r\n",
					"\r\n",
					"date_feats = [x['name'] for x in df_augmented.schema.jsonValue()['fields'] if x['type']=='date']\r\n",
					"\r\n",
					"df_date_int = df_augmented\r\n",
					"\r\n",
					"for feat in date_feats:\r\n",
					"    print(\"Casting date feature {} to int ...\".format(feat))\r\n",
					"    df_date_int = df_date_int.withColumn(feat+'_int', F.unix_timestamp(feat))\r\n",
					""
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# String indexers\r\n",
					"\r\n",
					"string_feats = [x['name'] for x in df_date_int.schema.jsonValue()['fields'] if x['type']=='string']\r\n",
					"\r\n",
					"df_string_indexed = df_date_int\r\n",
					"\r\n",
					"for feat in string_feats:\r\n",
					"    print(\"Indexing string feature {} ...\".format(feat))\r\n",
					"    if training:\r\n",
					"        indexer = StringIndexer(inputCol=feat, outputCol=feat+'_indexed', stringOrderType='frequencyDesc')\r\n",
					"        model = indexer.fit(df_string_indexed)\r\n",
					"        model.write().overwrite().save('_ijungle_indexer_'+feat+'.pkl')\r\n",
					"    else:\r\n",
					"        model = StringIndexerModel.load('_ijungle_indexer_'+feat+'.pkl')\r\n",
					"    df_string_indexed = model.transform(df_string_indexed)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Assemble features to scalate\r\n",
					"\r\n",
					"columns = df_string_indexed.schema.fieldNames()\r\n",
					"feats_to_remove = id_feat + date_feats + string_feats\r\n",
					"feats = [feat for feat in columns if not feat in feats_to_remove]\r\n",
					"assembler = VectorAssembler(inputCols=feats, outputCol='feats')\r\n",
					"df_assembled = assembler.transform(df_string_indexed).select(id_feat + ['feats'])"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Store features \r\n",
					"if training:\r\n",
					"    spark.createDataFrame(zip(range(len(feats)), feats),['id','feat']).write.mode('overwrite').parquet(features_path)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Scalate features\r\n",
					"if training:\r\n",
					"    scaler = StandardScaler(inputCol='feats', outputCol='scaled')\r\n",
					"    model = scaler.fit(df_assembled)\r\n",
					"    model.write().overwrite().save('_ijungle_scaler.pkl')\r\n",
					"else:\r\n",
					"    model = StandardScalerModel.load('_ijungle_scaler.pkl')\r\n",
					"df_scaled = model.transform(df_assembled).select(id_feat+['scaled'])"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Write scaled data as parquet files\r\n",
					"\r\n",
					"df_scaled.write.mode('overwrite').parquet(prepped_data_path)"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}
