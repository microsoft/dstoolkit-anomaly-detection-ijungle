{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# iJungle Tutorial Training Pipeline Example\r\n",
        "\r\n",
        "*TODO: Summary of the iJungle technique* \r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import iJungle\n",
        "from azureml.core import Workspace, Datastore, Dataset, Experiment, Environment, ScriptRunConfig\n",
        "import pandas as pd\n",
        "import os\n",
        "from azureml.core.compute import  ComputeTarget, AmlCompute\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.train.hyperdrive import GridParameterSampling, HyperDriveConfig, PrimaryMetricGoal, choice\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.pipeline.steps import PythonScriptStep, HyperDriveStep, HyperDriveStepRun\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "\n",
        "print(\"iJungle version:\", iJungle.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "iJungle version: 0.1.73\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1655239329501
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Parameters definition"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_name = \"cluster4\"\r\n",
        "environment_name = \"ijungle-training-env\"\r\n",
        "working_datastore_name = \"workspaceblobstore\"\r\n",
        "training_dataset_name = \"ijungle-trainining-dataset\"\r\n",
        "test_dataset_name = \"ijungle-test-dataset\"\r\n",
        "y_test_dataset_name = \"ijungle-y-test-dataset\"\r\n",
        "index_feature = 'index'\r\n",
        "pipeline_name = \"ijungle-training-pipeline\"\r\n",
        "subsample_list = [4096, 2048, 1024, 512]\r\n",
        "trees_list = [500, 100, 20, 10]\r\n",
        "train_expected_m = 50000\r\n",
        "overhead_expected_m = 50000\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655239329609
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preparation of cluster, environment and run configuration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "ws = Workspace.from_config()\r\n",
        "\r\n",
        "# Verify that cluster does not exist already\r\n",
        "try:\r\n",
        "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except:\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)\r\n",
        "    pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "\r\n",
        "# Creation of environment\r\n",
        "new_env = Environment(environment_name)\r\n",
        "packages = CondaDependencies.create(\r\n",
        "    conda_packages=['pip'],\r\n",
        "    pip_packages=['azureml-defaults','scikit-learn','pandas','pyarrow'])\r\n",
        "\r\n",
        "# Add iJungle library\r\n",
        "\r\n",
        "whl_filename = \"../dist/iJungle-\"+iJungle.__version__+\"-py3-none-any.whl\"\r\n",
        "\r\n",
        "whl_url = Environment.add_private_pip_wheel(workspace=ws,file_path = whl_filename, exist_ok=True)\r\n",
        "packages.add_pip_package(whl_url)\r\n",
        "\r\n",
        "\r\n",
        "# Add the dependencies to the environment\r\n",
        "new_env.python.conda_dependencies = packages\r\n",
        "\r\n",
        "# Register the environment \r\n",
        "new_env.register(workspace=ws)\r\n",
        "registered_env = Environment.get(ws, environment_name)\r\n",
        "\r\n",
        "# Create a new runconfig object for the pipeline\r\n",
        "pipeline_run_config = RunConfiguration()\r\n",
        "\r\n",
        "# Use the compute you created above. \r\n",
        "pipeline_run_config.target = pipeline_cluster\r\n",
        "\r\n",
        "# Assign the environment to the run configuration\r\n",
        "pipeline_run_config.environment = registered_env\r\n",
        "\r\n",
        "print (\"Run configuration created.\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it.\nRun configuration created.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655239330859
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 3. Data preparation and dataset registration\n",
        "\n",
        "*TODO: description of the data*\n",
        "\n",
        "1. Use the following data in this repository *TODO: KDD url to download the files*\n",
        "    - kddcup.names\n",
        "    - kddcup.data\n",
        "    - corrected"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Move to data directory\n",
        "os.chdir(os.path.dirname(os.path.abspath('__file__'))+'/../data')\n",
        "\n",
        "## Generate DataFrame with kdd data(csv format)\n",
        "names = list(pd.read_csv('kddcup.names',sep=':', header=None)[0])\n",
        "df = pd.read_csv('kddcup.data.gz', header=None, names=names)\n",
        "df_test = pd.read_csv('corrected.gz', header=None, names=names)\n",
        "\n",
        "print(\"Shape of raw data:\", df.shape)\n",
        "print(\"Shape of test data:\", df_test.shape)\n",
        "\n",
        "# Remove entries which protocol is not Http\n",
        "df = df[df.service == 'http']\n",
        "df_test = df_test[df_test.service == 'http']\n",
        "print(\"Shape of filtered train data:\", df.shape)\n",
        "print(\"Shape of filtered test data:\", df_test.shape)\n",
        "\n",
        "# Preparation of labels\n",
        "y_train = df.pop('label')\n",
        "y_test = df_test.pop('label')\n",
        "y_train = pd.Series([1 if val == 'normal.' else -1 for val in y_train], name=\"y\")\n",
        "y_test = pd.Series([1 if val == 'normal.' else -1 for val in y_test], name=\"y\")\n",
        "print(\"Shape of train labels:\", y_train.shape)\n",
        "print(\"Shape of test labels:\", y_test.shape)\n",
        "\n",
        "# Final preparation of training and testing data\n",
        "df.drop(['service'], axis=1, inplace=True)\n",
        "df_test.drop(['service'], axis=1, inplace=True)\n",
        "\n",
        "cat_columns = ['protocol_type', 'flag']\n",
        "\n",
        "for col in cat_columns:\n",
        "    df_test[col] = df_test[col].astype('category')\n",
        "    df[col] = df[col].astype('category')\n",
        "\n",
        "cat_columns = df.select_dtypes(['category']).columns\n",
        "df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
        "\n",
        "cat_columns = df_test.select_dtypes(['category']).columns\n",
        "df_test[cat_columns] = df_test[cat_columns].apply(lambda x: x.cat.codes)\n",
        "\n",
        "df.reset_index(inplace=True)\n",
        "df_test.reset_index(inplace=True)\n",
        "df_y_test = y_test.reset_index()\n",
        "\n",
        "print(\"Shape of train data:\", df.shape)\n",
        "print(\"Shape of test data:\", df_test.shape)\n",
        "print(\"Shape of y-test data:\", df_y_test.shape)\n",
        "\n",
        "datastore = Datastore.get(ws, working_datastore_name)\n",
        "\n",
        "print(\"Registering training dataset ...\")\n",
        "train_dataset = Dataset.Tabular.register_pandas_dataframe(df, datastore, training_dataset_name)\n",
        "\n",
        "print(\"Registering testing dataset ...\")\n",
        "test_dataset = Dataset.Tabular.register_pandas_dataframe(df_test, datastore, test_dataset_name)\n",
        "\n",
        "print(\"Registering y-testing dataset ...\")\n",
        "y_test_dataset = Dataset.Tabular.register_pandas_dataframe(df_y_test, datastore, y_test_dataset_name)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Shape of raw data: (4898431, 42)\nShape of test data: (311029, 42)\nShape of filtered train data: (623091, 42)\nShape of filtered test data: (41237, 42)\nShape of train labels: (623091,)\nShape of test labels: (41237,)\nShape of train data: (623091, 41)\nShape of test data: (41237, 41)\nShape of y-test data: (41237, 2)\nRegistering training dataset ...\nValidating arguments.\nArguments validated.\nSuccessfully obtained datastore reference and path.\nUploading file to managed-dataset/bd6448b8-8c72-4b54-be45-cd4e6ab5e212/\nSuccessfully uploaded file to datastore.\nCreating and registering a new dataset.\nSuccessfully created and registered a new dataset.\nRegistering testing dataset ...\nValidating arguments.\nArguments validated.\nSuccessfully obtained datastore reference and path.\nUploading file to managed-dataset/8d4f60e5-0127-4d71-8720-b0290a26ebce/\nSuccessfully uploaded file to datastore.\nCreating and registering a new dataset.\nSuccessfully created and registered a new dataset.\nRegistering y-testing dataset ...\nValidating arguments.\nArguments validated.\nSuccessfully obtained datastore reference and path.\nUploading file to managed-dataset/7fd41a9d-8dd1-4ebd-9a69-26cccf32c490/\nSuccessfully uploaded file to datastore.\nCreating and registering a new dataset.\nSuccessfully created and registered a new dataset.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655239359512
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Creation of training pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the training dataset\r\n",
        "train_ds = ws.datasets.get(training_dataset_name)\r\n",
        "\r\n",
        "# Intermadiate data\r\n",
        "dataprep_output = OutputFileDatasetConfig(\r\n",
        "    name=\"processed_data\", \r\n",
        "    destination=(\r\n",
        "        ws.datastores.get(working_datastore_name), \r\n",
        "        \"invoices/{run-id}/{output-name}\")\r\n",
        ").as_upload()\r\n",
        "\r\n",
        "# Step 1, Run the data prep script\r\n",
        "prep_step = PythonScriptStep(\r\n",
        "    name = \"Feature engineering Step\",\r\n",
        "    source_directory = \"../scripts\",\r\n",
        "    script_name = \"feat_eng.py\",\r\n",
        "    arguments = [\r\n",
        "        '--input-data', train_ds.as_named_input('input'),\r\n",
        "        '--prepped-data', dataprep_output,\r\n",
        "        '--index-feature', index_feature,    \r\n",
        "        '--training', 'True'    \r\n",
        "    ],\r\n",
        "    outputs=[dataprep_output],\r\n",
        "    compute_target = pipeline_cluster,\r\n",
        "    runconfig = pipeline_run_config,\r\n",
        "    allow_reuse = False\r\n",
        ")\r\n",
        "\r\n",
        "# Initial definition of the pipeline steps\r\n",
        "pipeline_steps = [prep_step]\r\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1655239360524
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next Step, run the training script\r\n",
        "\r\n",
        "dataprep_input = dataprep_output.as_input()\r\n",
        "node_count = int(pipeline_cluster.serialize()['properties']['properties']['scaleSettings']['maxNodeCount'])\r\n",
        "\r\n",
        "model_output_dir = OutputFileDatasetConfig(\r\n",
        "    name=\"model_output\", \r\n",
        "    destination=(\r\n",
        "        ws.datastores.get(working_datastore_name), \r\n",
        "        \"invoices/{run-id}/{output-name}\")\r\n",
        ").as_upload()\r\n",
        "\r\n",
        "script_config = ScriptRunConfig(\r\n",
        "    source_directory=\"../scripts\",\r\n",
        "    script=\"training.py\",\r\n",
        "    arguments = [\r\n",
        "        '--training-folder', dataprep_input,\r\n",
        "        '--max-subsample-size', max(subsample_list),\r\n",
        "        '--model-output', model_output_dir,\r\n",
        "        '--id-feat', index_feature,\r\n",
        "        '--train-expected-m', train_expected_m\r\n",
        "    ],\r\n",
        "    run_config = pipeline_run_config\r\n",
        ")\r\n",
        "\r\n",
        "params = GridParameterSampling(\r\n",
        "    {\r\n",
        "        '--trees': choice(trees_list),\r\n",
        "        '--subsample-size' : choice(subsample_list)\r\n",
        "    }\r\n",
        ")\r\n",
        "\r\n",
        "hyperdrive_config = HyperDriveConfig(\r\n",
        "    run_config = script_config, \r\n",
        "    hyperparameter_sampling = params, \r\n",
        "    policy = None, \r\n",
        "    primary_metric_name = 'Dummy', \r\n",
        "    primary_metric_goal = PrimaryMetricGoal.MAXIMIZE, \r\n",
        "    max_total_runs = len(trees_list)*len(subsample_list), \r\n",
        "    max_concurrent_runs = node_count\r\n",
        ") \r\n",
        "\r\n",
        "train_step = HyperDriveStep(\r\n",
        "    name = \"iJungle Trainining Step\", \r\n",
        "    hyperdrive_config = hyperdrive_config, \r\n",
        "    inputs=[dataprep_input],\r\n",
        "    outputs=[model_output_dir],\r\n",
        "    allow_reuse=False\r\n",
        ")\r\n",
        "\r\n",
        "pipeline_steps.append(train_step)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655239360652
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next step, overhead dataset calculation\r\n",
        "\r\n",
        "overhead_ds_output = OutputFileDatasetConfig(\r\n",
        "    name=\"overhead_ds_output\", \r\n",
        "    destination=(\r\n",
        "        ws.datastores.get(working_datastore_name), \r\n",
        "        \"invoices/{run-id}/{output-name}\")\r\n",
        ").as_upload()\r\n",
        "\r\n",
        "overhead_ds_step = PythonScriptStep(\r\n",
        "    name = \"Overhead Dataset Step\",\r\n",
        "    source_directory = \"../scripts\",\r\n",
        "    script_name = \"overhead_ds.py\",\r\n",
        "    arguments = [\r\n",
        "        '--input-data', dataprep_input,\r\n",
        "        '--overhead-data', overhead_ds_output,\r\n",
        "        '--overhead-expected-m', overhead_expected_m\r\n",
        "    ],\r\n",
        "    inputs=[dataprep_input],\r\n",
        "    outputs=[overhead_ds_output],\r\n",
        "    compute_target = pipeline_cluster,\r\n",
        "    runconfig = pipeline_run_config,\r\n",
        "    allow_reuse = False\r\n",
        ")\r\n",
        "pipeline_steps.append(overhead_ds_step)\r\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655239360833
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next step, run the overhead script\r\n",
        "\r\n",
        "model_input_dir = model_output_dir.as_input()\r\n",
        "overhead_ds_input = overhead_ds_output.as_input()\r\n",
        "\r\n",
        "overhead_output = OutputFileDatasetConfig(\r\n",
        "    name=\"overhead_output\", \r\n",
        "    destination=(\r\n",
        "        ws.datastores.get(working_datastore_name), \r\n",
        "        \"invoices/{run-id}/{output-name}\")\r\n",
        ").as_upload()\r\n",
        "\r\n",
        "script_config = ScriptRunConfig(\r\n",
        "    source_directory=\"../scripts\",\r\n",
        "    script=\"overhead.py\",\r\n",
        "    arguments = [\r\n",
        "        '--overhead-folder', overhead_ds_input,\r\n",
        "        '--model-input', model_input_dir,\r\n",
        "        '--overhead-output', overhead_output,\r\n",
        "        '--id-feat', index_feature\r\n",
        "        ],\r\n",
        "    run_config = pipeline_run_config\r\n",
        ")\r\n",
        "\r\n",
        "params = GridParameterSampling(\r\n",
        "    {\r\n",
        "        '--trees': choice(trees_list),\r\n",
        "        '--subsample-size' : choice(subsample_list)\r\n",
        "    }\r\n",
        ")\r\n",
        "\r\n",
        "hyperdrive_config = HyperDriveConfig(\r\n",
        "    run_config = script_config, \r\n",
        "    hyperparameter_sampling = params, \r\n",
        "    policy = None, \r\n",
        "    primary_metric_name = 'Dummy', \r\n",
        "    primary_metric_goal = PrimaryMetricGoal.MAXIMIZE, \r\n",
        "    max_total_runs = len(trees_list)*len(subsample_list), \r\n",
        "    max_concurrent_runs = node_count\r\n",
        ") \r\n",
        "\r\n",
        "overhead_step = HyperDriveStep(\r\n",
        "    name = \"iJungle Overhead Step\", \r\n",
        "    hyperdrive_config = hyperdrive_config, \r\n",
        "    inputs=[overhead_ds_input, model_input_dir],\r\n",
        "    outputs=[overhead_output],\r\n",
        "    allow_reuse=False\r\n",
        ")\r\n",
        "\r\n",
        "pipeline_steps.append(overhead_step)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655239360929
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next steps, find the representative iForest\r\n",
        "\r\n",
        "overhead_input = overhead_output.as_input()\r\n",
        "\r\n",
        "best_iforest_step = PythonScriptStep(\r\n",
        "    name = \"Best iForest Step\",\r\n",
        "    source_directory = \"../scripts\",\r\n",
        "    script_name = \"best_iforest.py\",\r\n",
        "    arguments = [\r\n",
        "        '--overhead-input', overhead_input,\r\n",
        "        '--subsample-list', str(subsample_list),\r\n",
        "        '--trees-list', str(trees_list)\r\n",
        "    ],\r\n",
        "    inputs=[overhead_input],\r\n",
        "    compute_target = pipeline_cluster,\r\n",
        "    runconfig = pipeline_run_config,\r\n",
        "    allow_reuse = False\r\n",
        ")\r\n",
        "pipeline_steps.append(best_iforest_step)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655239361026
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the pipeline\r\n",
        "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\r\n",
        "print(\"Pipeline is built.\")\r\n",
        "\r\n",
        "# Create an experiment and run the pipeline\r\n",
        "experiment = Experiment(workspace=ws, name = pipeline_name)\r\n",
        "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\r\n",
        "print(\"Pipeline submitted for execution.\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline is built.\nCreated step Feature engineering Step [1dab3f3a][da82670e-3f7e-4be0-88f6-507debe692f6], (This step will run and generate new outputs)\nCreated step iJungle Trainining Step [6b78089a][3c5b501f-987f-4dea-adae-e9613855b04c], (This step will run and generate new outputs)Created step Overhead Dataset Step [03970c0c][d8d83657-f3f1-4806-97c6-f40170b8f0b0], (This step will run and generate new outputs)\n\nCreated step iJungle Overhead Step [5e5520f5][074c153a-7a82-4e83-96da-6e62b1d5cedb], (This step will run and generate new outputs)\nCreated step Best iForest Step [26f37c61][5c436f27-c6b8-42b9-a0bd-5b91a3ea06eb], (This step will run and generate new outputs)\nSubmitted PipelineRun d571febb-ba82-4190-874a-4823dd9e978d\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/d571febb-ba82-4190-874a-4823dd9e978d?wsid=/subscriptions/d412dac0-d902-4cfb-b2f9-19dea115f7ff/resourcegroups/rg-dv-aidnaanomaly-corp-eus2/workspaces/wsmldvanomaly&tid=973ba820-4a58-4246-84bf-170e50b3152a\nPipeline submitted for execution.\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655239374520
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}