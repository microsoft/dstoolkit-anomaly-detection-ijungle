{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DO NOT START THIS NOTEBOOK UNTIL THE PIPELINE CREATED IN THE PREVIOUS STEP, THE IJUNGLE TRAINING PIPELINE, IS IN \"COMPLETE\" STATUS.\r\n",
        "\r\n",
        "# iJungle Inference pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Environment, Experiment, ScriptRunConfig\r\n",
        "from azureml.core.compute import ComputeTarget\r\n",
        "from azureml.core.conda_dependencies import CondaDependencies\r\n",
        "from azureml.core.runconfig import RunConfiguration\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "from azureml.pipeline.steps import PythonScriptStep, ParallelRunConfig, ParallelRunStep\r\n",
        "from azureml.data import OutputFileDatasetConfig"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655246932882
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_name = \"cluster4\"\r\n",
        "environment_name = \"ijungle-inference-env\"\r\n",
        "input_dataset_name=\"ijungle-test-dataset\"\r\n",
        "working_datastore_name=\"workspaceblobstore\"\r\n",
        "output_datastore_name=\"workspaceblobstore\"\r\n",
        "output_path=\"iJungle/results/\"\r\n",
        "pipeline_name=\"ijungle-inference-pipeline\"\r\n",
        "\r\n",
        "index_feature = 'index'\r\n",
        "anomaly_score = -.8"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655246933174
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\r\n",
        "pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "print('Cluster configured to execute the pipeline:',cluster_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Cluster configured to execute the pipeline: cluster4\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655246934825
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_env = Environment(environment_name)\r\n",
        "packages = CondaDependencies.create(\r\n",
        "    conda_packages=['pip'],\r\n",
        "    pip_packages=['azureml-defaults','azureml-interpret','scikit-learn','pandas','pyarrow'])\r\n",
        "\r\n",
        "# Add the dependencies to the environment\r\n",
        "new_env.python.conda_dependencies = packages\r\n",
        "\r\n",
        "# Register the environment \r\n",
        "new_env.register(workspace=ws)\r\n",
        "registered_env = Environment.get(ws, environment_name)\r\n",
        "\r\n",
        "# Create a new runconfig object for the pipeline\r\n",
        "pipeline_run_config = RunConfiguration()\r\n",
        "\r\n",
        "# Use the compute you created above. \r\n",
        "pipeline_run_config.target = pipeline_cluster\r\n",
        "\r\n",
        "# Assign the environment to the run configuration\r\n",
        "pipeline_run_config.environment = registered_env\r\n",
        "\r\n",
        "print (\"Run configuration created.\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Run configuration created.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655246935455
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the inference dataset\r\n",
        "inference_ds = ws.datasets.get(input_dataset_name)\r\n",
        "\r\n",
        "# Intermadiate data\r\n",
        "dataprep_output = OutputFileDatasetConfig(\r\n",
        "    name=\"processed_data\", \r\n",
        "    destination=(\r\n",
        "        ws.datastores.get(working_datastore_name), \r\n",
        "        \"invoices/{run-id}/{output-name}\")\r\n",
        ").as_upload()\r\n",
        "\r\n",
        "# Step 1, Run the data prep script\r\n",
        "prep_step = PythonScriptStep(\r\n",
        "    name = \"Inference data preparation Step\",\r\n",
        "    source_directory = \"../scripts\",\r\n",
        "    script_name = \"feat_eng.py\",\r\n",
        "    arguments = [\r\n",
        "        '--input-data', inference_ds.as_named_input('input'),\r\n",
        "        '--prepped-data', dataprep_output,\r\n",
        "        '--index-feature', index_feature,\r\n",
        "        '--training', 'False',        \r\n",
        "    ],\r\n",
        "    outputs=[dataprep_output],\r\n",
        "    compute_target = pipeline_cluster,\r\n",
        "    runconfig = pipeline_run_config,\r\n",
        "    allow_reuse = False\r\n",
        ")\r\n",
        "\r\n",
        "# Initial definition of the pipeline steps\r\n",
        "pipeline_steps = [prep_step]\r\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655246936585
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next Step, run the inferencing script\r\n",
        "\r\n",
        "node_count = int(pipeline_cluster.serialize()['properties']['properties']['scaleSettings']['maxNodeCount'])\r\n",
        "\r\n",
        "dataprep_input = dataprep_output.read_parquet_files().as_input(\"inference_data\")\r\n",
        "\r\n",
        "inference_output_dir = OutputFileDatasetConfig(\r\n",
        "    name=\"inference_output\", \r\n",
        "    destination=(\r\n",
        "        ws.datastores.get(working_datastore_name), \r\n",
        "        \"invoices/{run-id}/{output-name}\")\r\n",
        ").as_upload()\r\n",
        "\r\n",
        "inference_step = PythonScriptStep(\r\n",
        "    name = \"Inference Step\",\r\n",
        "    source_directory = \"../scripts\",\r\n",
        "    script_name = \"inference.py\",\r\n",
        "    arguments = [\r\n",
        "        '--input', dataprep_input,\r\n",
        "        '--output', inference_output_dir,\r\n",
        "        '--feat-id', index_feature\r\n",
        "    ],\r\n",
        "    inputs=[dataprep_input],\r\n",
        "    outputs=[inference_output_dir],\r\n",
        "    compute_target = pipeline_cluster,\r\n",
        "    runconfig = pipeline_run_config,\r\n",
        "    allow_reuse = False\r\n",
        ")\r\n",
        "\r\n",
        "pipeline_steps.append(inference_step)\r\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655246938497
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next step, explainability\r\n",
        "\r\n",
        "interpret_input = inference_output_dir.read_parquet_files().as_input(\"interpret_input\")\r\n",
        "\r\n",
        "interpret_output_dir = OutputFileDatasetConfig(\r\n",
        "    name=\"interpret_output\", \r\n",
        "    destination=(\r\n",
        "        ws.datastores.get(output_datastore_name), \r\n",
        "        output_path)\r\n",
        ").as_upload()\r\n",
        "\r\n",
        "\r\n",
        "interpret_step = PythonScriptStep(\r\n",
        "    name = \"Explainability Step\",\r\n",
        "    source_directory = \"../scripts\",\r\n",
        "    script_name = \"interpret.py\",\r\n",
        "    arguments = [\r\n",
        "        '--input', interpret_input,\r\n",
        "        '--dataprep', dataprep_input,\r\n",
        "        '--output', interpret_output_dir,\r\n",
        "        '--index-id', index_feature,\r\n",
        "        '--anomaly-score', anomaly_score\r\n",
        "    ],\r\n",
        "    inputs=[ interpret_input, dataprep_input],\r\n",
        "    outputs=[interpret_output_dir],\r\n",
        "    compute_target = pipeline_cluster,\r\n",
        "    runconfig = pipeline_run_config,\r\n",
        "    allow_reuse = False\r\n",
        ")\r\n",
        "pipeline_steps.append(interpret_step)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataprep_output_outliers' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d0664c56f92d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minterpret_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_output_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"interpret_input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataprep_input_outliers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataprep_output_outliers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataprep_input_outliers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m interpret_output_dir = OutputFileDatasetConfig(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataprep_output_outliers' is not defined"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655246938617
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the pipeline\r\n",
        "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\r\n",
        "print(\"Pipeline is built.\")\r\n",
        "\r\n",
        "# Create an experiment and run the pipeline\r\n",
        "experiment = Experiment(workspace=ws, name = pipeline_name)\r\n",
        "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\r\n",
        "print(\"Pipeline submitted for execution.\")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655246938658
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}