{
	"name": "03 - ijungle - training - overhead",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "17d09700-3932-4386-ae95-5fcc8f410a21"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"from pyspark.ml.functions import vector_to_array\r\n",
					"import numpy as np"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"prepped_data_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/02-Prepped_Data'\r\n",
					"iFor_data_prefix = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/03-iFor'\r\n",
					"overhead_data_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/04-Overhead_Data'\r\n",
					"overhead_results_prefix = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/05-iFor_results'\r\n",
					"\r\n",
					"subsample_list = \"[4096, 2048, 1024, 512]\"\r\n",
					"trees_list = \"[500, 100, 20, 10]\"\r\n",
					"train_size = \"0.01\"\r\n",
					"id_feat = \"['issuer_id','issued_date']\"\r\n",
					"id_feat_types = \"['int', 'date']\"\r\n",
					"seed = \"42\"\r\n",
					"overhead_size = \"0.01\""
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Casting parameters\r\n",
					"\r\n",
					"subsample_list = eval(subsample_list)\r\n",
					"trees_list = eval(trees_list)\r\n",
					"train_size = float(train_size)\r\n",
					"id_feat = eval(id_feat)\r\n",
					"id_feat_types = eval(id_feat_types)\r\n",
					"seed = int(seed)\r\n",
					"overhead_size = float(overhead_size)"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df = spark.read.parquet(prepped_data_path)\r\n",
					"m = df.count()\r\n",
					"print(\"Number of records: {:,}\".format(m))"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"max_subsample_size = max(subsample_list)\r\n",
					"num_groups = int(np.ceil(m*train_size/max_subsample_size))\r\n",
					"print(\"Num groups: \", num_groups)"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Creation of overhead sample\r\n",
					"\r\n",
					"df_W = df.sample(withReplacement=False, fraction=overhead_size)\r\n",
					"df_W.write.mode('overwrite').parquet(overhead_data_path)\r\n",
					"df_W = spark.read.parquet(overhead_data_path)\r\n",
					"\r\n",
					"num_feats = len(df_W.head(1)[0]['scaled'])\r\n",
					"df_W_unassembled = df_W.withColumn('f', vector_to_array(\"scaled\")).select(id_feat + [F.col(\"f\")[i] for i in range(num_feats)])\r\n",
					"print(\"Number of records of overhead dataset: {:,}\".format(df_W_unassembled.count()))"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def ijungle_overhead(id_feat, subsample_size, trees, model_bytes, group):\r\n",
					"    def _fun(iterator):\r\n",
					"        import pandas as pd\r\n",
					"        import joblib\r\n",
					"\r\n",
					"        model_filename = 'iJungle_' + str(group) + '_' + str(trees) + '_' + str(subsample_size) + '.pkl'\r\n",
					"\r\n",
					"        with open(model_filename, 'wb') as model_file:\r\n",
					"            model_file.write(model_bytes)\r\n",
					"\r\n",
					"        clf = joblib.load(model_filename)\r\n",
					"\r\n",
					"        for pdf in iterator:\r\n",
					"            pdf.set_index(id_feat, inplace=True)\r\n",
					"            _predict = clf.predict(pdf)\r\n",
					"            pdf.reset_index(drop=False, inplace=True)\r\n",
					"            pdf_out = pd.DataFrame()\r\n",
					"            pdf_out[id_feat] = pdf[id_feat]\r\n",
					"            pdf_out['predict_'+str(group)] = _predict\r\n",
					"            yield(pdf_out)\r\n",
					"    return(_fun)"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"dcc_str = \", \".join([x[0]+\" \"+x[1] for x in zip(id_feat, id_feat_types)])\r\n",
					"\r\n",
					"\r\n",
					"for trees in trees_list:\r\n",
					"    for subsample_size in subsample_list:\r\n",
					"        print(\"Overhead calculation for trees {}, and subsample size {} ...\".format(trees, subsample_size))\r\n",
					"        iFor_data_path = iFor_data_prefix + '_' + str(trees) + '_' + str(subsample_size)\r\n",
					"        df_iFor = spark.read.parquet(iFor_data_path)\r\n",
					"        df_predict = df_W_unassembled.select(id_feat)\r\n",
					"        for group in range(num_groups):\r\n",
					"            model_bytes = df_iFor.where(F.col('id')==group).select('model').collect()[0]['model']\r\n",
					"            df_predict_group =df_W_unassembled.mapInPandas(\r\n",
					"                ijungle_overhead(id_feat, subsample_size, trees, model_bytes, group), \r\n",
					"                schema=dcc_str + \", predict_\"+str(group)+\" float\"\r\n",
					"            )\r\n",
					"            df_predict = df_predict.join(df_predict_group, on=id_feat)\r\n",
					"        df_predict.write.mode('overwrite').parquet(overhead_results_prefix + '_' + str(trees) + '_' + str(subsample_size))\r\n",
					""
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}