{
	"name": "04 - ijungle - training - best iforest",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d4734a04-fba7-48e0-ac7f-0058d606f4e5"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"import numpy as np"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"iFor_data_prefix = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/03-iFor'\r\n",
					"overhead_data_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/04-Overhead_Data'\r\n",
					"overhead_results_prefix = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/05-iFor_results'\r\n",
					"best_iforest_path = 'abfss://pridevsynapseworkingfs@eiadapstoreworking.dfs.core.windows.net/ijungle/06-best_iforest'\r\n",
					"subsample_list = \"[4096, 2048, 1024, 512]\"\r\n",
					"trees_list = \"[500, 100, 20, 10]\"\r\n",
					"id_feat = \"['issuer_id','issued_date']\""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Casting parameters\r\n",
					"\r\n",
					"subsample_list = eval(subsample_list)\r\n",
					"trees_list = eval(trees_list)\r\n",
					"id_feat = eval(id_feat)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df = spark.read.parquet(overhead_data_path).select(id_feat)\r\n",
					"print(\"Overhead dataset, number of records: {:,}\".format(df.count()))\r\n",
					"avg_feat_names = []\r\n",
					"for trees in trees_list:\r\n",
					"    for subsample_size in subsample_list:\r\n",
					"        print(\"Calculating average result for {} trees and {} subsample size\".format(trees, subsample_size))\r\n",
					"        overhead_results_path = overhead_results_prefix + '_' + str(trees) + '_' + str(subsample_size)\r\n",
					"        df_predict = spark.read.parquet(overhead_results_path)\r\n",
					"        feats = [feat for feat in df_predict.columns if not feat in id_feat] \r\n",
					"        num_groups = len(feats)\r\n",
					"        average_fun = sum(map(F.col, feats))/len(feats)\r\n",
					"        avg_feat_name = 'avg' + '_' + str(trees) + '_' + str(subsample_size)\r\n",
					"        avg_feat_names.append(avg_feat_name)\r\n",
					"        df_average = df_predict.withColumn(avg_feat_name,average_fun).select(id_feat + [avg_feat_name])\r\n",
					"        df = df.join(df_average, on=id_feat)\r\n",
					"average_fun = sum(map(F.col, avg_feat_names))/len(avg_feat_names)\r\n",
					"df = df.withColumn('avg',average_fun).select(id_feat + ['avg'])"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"pdf = df.toPandas().set_index(id_feat)\r\n",
					"best_l2 = np.inf\r\n",
					"\r\n",
					"for trees in trees_list:\r\n",
					"    for subsample_size in subsample_list:\r\n",
					"        print(\"Calculating the best iForest analyzing {} trees and {} subsample size\".format(trees, subsample_size))\r\n",
					"        overhead_results_path = overhead_results_prefix + '_' + str(trees) + '_' + str(subsample_size)\r\n",
					"        df_predict = spark.read.parquet(overhead_results_path)\r\n",
					"        for group in range(num_groups):\r\n",
					"            predict_feat = 'predict_'+str(group)\r\n",
					"            pdf_predict = df_predict.select(id_feat + [predict_feat]).toPandas().set_index(id_feat)\r\n",
					"            pdf_joined = pdf.join(pdf_predict)\r\n",
					"            l2 = np.linalg.norm(pdf_joined[predict_feat]-pdf_joined['avg'])\r\n",
					"            if l2 < best_l2:\r\n",
					"                best_l2 = l2\r\n",
					"                best_trees = trees\r\n",
					"                best_subsample_size = subsample_size\r\n",
					"                best_group = group\r\n",
					"print(\"Best trees\", best_trees)\r\n",
					"print(\"Best subsample size\", best_subsample_size)\r\n",
					"print(\"Best group\", best_group)\r\n",
					""
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"iFor_data_path = iFor_data_prefix + '_' + str(best_trees) + '_' + str(best_subsample_size)\r\n",
					"df_iFor = spark.read.parquet(iFor_data_path)\r\n",
					"model_bytes = df_iFor.where(F.col('id')==best_group).select('model').collect()[0]['model']"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"spark.createDataFrame([('best_iforest',model_bytes)],schema=['id','model']).write.mode('overwrite').parquet(best_iforest_path)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}